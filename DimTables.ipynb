{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6154996d-87df-4294-8ded-25d09b98962b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 1. Creating dimension city table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8f2bd167-51d4-4164-98cd-31981f7f8517","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\n\nCREATE TABLE IF NOT EXISTS dim_city (\n    city_id INT,\n    city_name STRING,\n    country STRING,\n    latitude float,\n    longitude float\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"nuid":"fd58d9da-7caf-40ba-8507-eb7e465cdda7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Inserting the data into the dim_city from the cleaned_weather table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9c58def0-a5dc-4cdd-82a3-d2b3c1be1c92","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_city():\n    data = spark.sql(\"\"\"\n                        SELECT DISTINCT city_id, city_name, country, lat, lon\n                        FROM weather_processed\n                     \"\"\")\n\n    # Enable schema migration\n    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n\n    # Insert the selected data into the dim_city table\n    data.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"dim_city\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"nuid":"b8eb5f73-ab92-4586-bbee-0ac9e7446c2e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2. Creating the date dimension table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8e352165-01f4-48e8-9232-568fcdb4aaeb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Inserting data into dim_date table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b07dccfe-c8b2-42d0-bc0c-bb610bd28864","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_date(start_date='2023-01-01', end_date='2023-12-31'):\n  \n    # importing necessary libraries\n    from pyspark.sql import functions as F\n    from pyspark.sql.types import IntegerType\n    from pyspark.pandas import date_range\n \n    # creating daterange\n    date_df = spark.createDataFrame(date_range(start=start_date, end=end_date).to_numpy(), ['Date'])\n    # creating other columns\n    date_df = date_df.withColumn('dateID', F.date_format('Date', 'yyyyMMdd').cast(IntegerType()))\n    date_df = date_df.withColumn('fullDate', F.date_format('Date', 'yyyy-MM-dd').cast('DATE'))\n    date_df = date_df.withColumn('monthName', F.date_format('Date', 'MMMM'))\n    date_df = date_df.withColumn('monthNumOfYear', F.month('Date'))\n    date_df = date_df.withColumn('dayNameOfWeek', F.date_format('Date', 'EEEE'))\n    date_df = date_df.withColumn('dayNumOfWeek', F.dayofweek('Date'))\n    date_df = date_df.withColumn('dayNumOfMonth', F.dayofmonth('Date'))\n    date_df = date_df.withColumn('dayNumOfYear', F.dayofyear('Date'))\n    date_df = date_df.withColumn('weekNumOfYear', F.weekofyear('Date'))\n    date_df = date_df.withColumn('quarterName', F.concat(F.lit('Q').cast('string'), F.quarter('Date')))\n    date_df = date_df.withColumn('calenderQuarter', F.quarter('Date'))\n    date_df = date_df.withColumn('calenderYear', F.year('Date'))\n    date_df = date_df.drop('Date')\n \n    # loading into dim_date table\n    date_df.write.format('delta').mode('overwrite').saveAsTable('dim_date')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"nuid":"ad263168-0b11-4324-98a4-4b564df44806","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3. Creating the time dimension table and generating the time"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"56c3315d-623a-4b56-8f50-8d2a5c2defba","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_time():\n \n    from pyspark.sql.functions import col\n \n    time_ranges = []\n    \n    for hour in range(24):\n        start_time = f\"{hour:02d}:00\"  # Format start time as HH:00\n        end_time = f\"{hour:02d}:59\"  # Format end time as HH:59\n        time_ranges.append({\"timeID\": hour, \"startTime\": start_time, \"endTime\": end_time})\n    \n    dfs = spark.createDataFrame(time_ranges)\n    dfs = dfs.withColumn('timeID', col('timeID').cast('int'))\n    dfs.write.format('delta').mode('overwrite').saveAsTable('dim_time')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7ffe835e-7049-4fb8-ada2-5dc74ad4723e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DimTables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":206133631172161,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
