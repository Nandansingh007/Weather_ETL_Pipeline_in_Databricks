{"cells":[{"cell_type":"markdown","source":["#### Importing Necessary Modules"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"28fae2d9-83a3-4235-8b97-292fdc2ac45e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import datetime\nfrom pyspark.sql.functions import split, date_format, hour,lit\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, row_number"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8c561562-e437-401e-ad3d-a05f08d09c49","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Creation of hourly_fact_table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85a501ca-3389-4275-85b3-328987ddbb86","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["@keep_log\ndef get_hourly_fact_weather(df):\n    \n    from datetime import datetime\n    from pyspark.sql.functions import hour, col\n    \n    df = df.withColumn('timeID', hour('created_on')).withColumn('Date', col('created_on').cast('date'))\n    \n    try:\n        # df_hourly_fact = spark.table('hourly_fact_weather')\n        timeID, date = df.select('timeID', 'Date').first()\n        dateID = str(date).replace('-', '')\n        query = f\"delete from hourly_fact_weather where timeID='{timeID}' and dateID='{dateID}';\"\n        spark.sql(query)\n    except:\n        pass\n    \n    df_date = spark.table('dim_date')\n    df_fact = df.join(df_date, df.Date == df_date.fullDate).select(\n                            df.timeID,\n                            df_date.dateID,\n                            df.city_id,\n                            df.temp,\n                            df.temp_min,\n                            df.temp_max,\n                            df.pressure,\n                            df.humidity,\n                            df.visibility,\n                            df.wind_speed,\n                            df.wind_deg,\n                            df.wind_gust,\n                            df.clouds_all\n                        )\n    start = datetime.fromtimestamp(df.selectExpr(\"min(dt)\").first()[0])\n    end = datetime.fromtimestamp(df.selectExpr(\"max(dt)\").first()[0])\n    \n    return df_fact, start, end"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"898f3dbb-938e-46d4-aa80-1e9a66b1c833","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Creation of daily_fact_table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eba48cce-e999-4653-9943-1bbf95b7bdb6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["@keep_log\ndef get_daily_fact_weather(df):\n    \n    from datetime import datetime\n    from pyspark.sql.functions import hour, col, min, max, mean\n    \n    df = df.withColumn('Date', col('created_on').cast('date'))  \n    \n    df_date = spark.table('dim_date')\n    \n    df_fact = df.join(df_date, df.Date == df_date.fullDate).select(\n                            df_date.dateID,\n                            df.city_id,\n                            df.temp,\n                            df.temp_min,\n                            df.temp_max,\n                            df.pressure,\n                            df.humidity,\n                            df.visibility,\n                            df.wind_speed,\n                            df.wind_deg,\n                            df.wind_gust,\n                            df.clouds_all\n                        )\n    \n    df_daily_fact = df_fact.alias('df_daily_fact')\n    \n    try:\n        date = df.select('Date').first()[0]\n        dateID = str(date).replace('-', '')\n#         print(\"============>>>\", dateID)\n        df_daily_fact = spark.sql(f\"select * from daily_fact_weather where dateID='{dateID}';\")\\\n                            .drop('load_run_id', 'created_on', 'created_by')\n        \n#         display(df_daily_fact)\n        \n        spark.sql(f\"delete from daily_fact_weather where dateID='{dateID}';\")\n    except:\n        df_daily_fact = df_fact.alias('df_daily_fact')\n    \n    \n    df_fact = df_fact.union(df_daily_fact).groupBy(col('city_id'), col('dateID'))\\\n                    .agg(mean(\"temp\").alias('temp'), \n                         min('temp_min').alias('temp_min'), \n                         max('temp_max').alias('temp_max'), \n                         mean('pressure').cast('int').alias('pressure'), \n                         mean('humidity').cast('int').alias('humidity'), \n                         mean('visibility').cast('int').alias('visibility'),\n                         mean('wind_speed').cast('int').alias('wind_speed'),\n                         mean('wind_deg').cast('int').alias('wind_deg'), \n                         mean('wind_gust').alias('wind_gust'),\n                         mean('clouds_all').alias('clouds_all'),\n                        )\n    \n    \n    start = datetime.fromtimestamp(df.selectExpr(\"min(dt)\").first()[0])\n    end = datetime.fromtimestamp(df.selectExpr(\"max(dt)\").first()[0])\n    \n    return df_fact, start, end"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a47faa9-023a-4d0a-b5e3-74534b5b0a39","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"FactTables","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":5659525501250,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
